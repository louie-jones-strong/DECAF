{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Formatting\n",
    "\n",
    "This notebook was used for collecting and formatting the results of the experiments, for the evaluation section of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import src.Common.Utils.Config.ConfigHelper as ConfigHelper\n",
    "import shutil\n",
    "import src.Common.EpisodeReplay.EpisodeReplay as EpisodeReplay\n",
    "from tqdm import tqdm\n",
    "import pyperclip as pc\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunGroup = \"13\"\n",
    "EnvNames = [\"FrozenLake\"]\n",
    "BehaviouralTypes = [\"Human\", \"Curated\", \"HighScore\"]\n",
    "BehaviouralTypesToReview = [\"Human\", \"Curated\"]\n",
    "AgentTypes = [\"HardCoded\", \"ML\", \"Random\", \"Human\"]\n",
    "\n",
    "# manual Review of the results config\n",
    "MaxChoicesPerAgent = 5\n",
    "MaxReplaysPerChoice = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied Human Demos for: FrozenLake\n",
      "Added 59 Human episodes to FrozenLake\n",
      "Added 23 Curated episodes to FrozenLake\n",
      "Added 82 HighScore episodes to FrozenLake\n"
     ]
    }
   ],
   "source": [
    "# copy demos of each behavioural type to the run group folder\n",
    "def CopyDemos(envName, runGroup):\n",
    "\tfromPath = os.path.join(\"Data\", envName, \"dev\", \"replays\", \"Human\")\n",
    "\ttoPath = os.path.join(\"Data\", envName, runGroup, \"replays\", \"Human\")\n",
    "\n",
    "\tif os.path.exists(toPath):\n",
    "\t\tshutil.rmtree(toPath)\n",
    "\tshutil.copytree(fromPath, toPath)\n",
    "\n",
    "\tprint(\"Copied Human Demos for: \" + envName)\n",
    "\treturn\n",
    "\n",
    "def AddDemoIdsToBehaviour(envName, runGroup, behaviourType):\n",
    "\t# load the results\n",
    "\tstatsPath = os.path.join(\"Data\", envName, runGroup, \"replays\", \"Human\", \"stats.tsv\")\n",
    "\tstats = pd.read_csv(statsPath, sep=\"\\t\")\n",
    "\n",
    "\tlowerBehaviourType = behaviourType.lower()\n",
    "\n",
    "\tstats[\"Behaviour\"] = stats[\"metricName\"].apply(lambda x: x.split(\"_\")[-2])\n",
    "\n",
    "\tif lowerBehaviourType != \"highscore\":\n",
    "\t\tstats = stats[stats[\"Behaviour\"] == lowerBehaviourType]\n",
    "\n",
    "\tepisodeIds = stats[\"EpisodeId\"].unique().tolist()\n",
    "\n",
    "\tmetricName = \"Human_\" + behaviourType\n",
    "\n",
    "\t# load the json with the episode Ids of the behavioural type\n",
    "\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\t\n",
    "\tepisodeIdsJson = ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\tepisodeIdsJson[metricName] = episodeIds\n",
    "\n",
    "\tConfigHelper.SaveConfig(episodeIdsJson, episodeIdsPath)\n",
    "\n",
    "\tprint(f\"Added {len(episodeIds)} {behaviourType} episodes to {envName}\")\n",
    "\treturn\n",
    "\n",
    "for envName in EnvNames:\n",
    "\tCopyDemos(envName, RunGroup)\n",
    "\n",
    "\tfor behaviourType in BehaviouralTypes:\n",
    "\t\tAddDemoIdsToBehaviour(envName, RunGroup, behaviourType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Replays For Manual Reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadReplay(envName, runGroup, agentType, episodeId):\n",
    "\t\n",
    "\tpath = os.path.join(\"Data\", envName, runGroup, \"replays\", agentType, episodeId)\n",
    "\ttry:\n",
    "\t\treplay = EpisodeReplay.EpisodeReplay.LoadFromFolder(path)\n",
    "\t\treturn replay\n",
    "\texcept:\n",
    "\t\treturn None\n",
    "\n",
    "def CollectReplaysToReview(envName, runGroup, behaviourType):\n",
    "\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\treplays =  ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\n",
    "\tcolumns = [\"AgentId\", \"Predicted\", \"AgentType\"]\n",
    "\tcolumns += [f\"Replay_{i}\" for i in range(MaxReplaysPerChoice)]\n",
    "\n",
    "\treplaysToReview = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor agentId, episodeIds in replays.items():\n",
    "\t\t\n",
    "\t\tagentType = agentId.split(\"_\")[0]\n",
    "\t\tids = []\n",
    "\n",
    "\t\tfor i in range(len(episodeIds)):\n",
    "\t\t\tepisodeId = episodeIds[i]\n",
    "\n",
    "\t\t\treplay = LoadReplay(envName, runGroup, agentType, episodeId)\n",
    "\t\t\tif replay is None:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tids.append(episodeId)\n",
    "\n",
    "\t\t\tif len(ids) >= MaxReplaysPerChoice or i == len(episodeIds) - 1:\n",
    "\t\t\t\trow = {}\n",
    "\t\t\t\trow[\"AgentId\"] = [agentId]\n",
    "\t\t\t\trow[\"Predicted\"] = [None]\n",
    "\t\t\t\trow[\"AgentType\"] = [agentType]\n",
    "\t\t\t\tfor i, id in enumerate(ids):\n",
    "\t\t\t\t\trow[f\"Replay_{i}\"] = [id]\n",
    "\n",
    "\t\t\t\treplaysToReview = pd.concat([replaysToReview, pd.DataFrame(row)], ignore_index=True)\n",
    "\t\t\t\tids = []\n",
    "\n",
    "\treturn replaysToReview\n",
    "\t\n",
    "for envName in EnvNames:\n",
    "\tfor behaviourType in BehaviouralTypesToReview:\n",
    "\n",
    "\t\treplaysToReview = CollectReplaysToReview(envName, RunGroup, behaviourType)\n",
    "\t\treplaysToReview = replaysToReview.sample(frac=1)\n",
    "\t\treplaysToReviewPath = os.path.join(\"Data\", envName, RunGroup, f\"ReplaysToReview_{behaviourType}.json\")\n",
    "\t\treplaysToReview.to_json(replaysToReviewPath, orient=\"records\", indent=4)\n",
    "\n",
    "\t\tprint(f\"Collected {len(replaysToReview)} replays to review for {behaviourType} in {envName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formate the results of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateBinaryCI(positives, totalCount, confidenceLevel=0.95):\n",
    "\t# Calculate the sample proportion (p)\n",
    "\tp = positives / totalCount\n",
    "\t\n",
    "\t# Calculate the standard error\n",
    "\tse = math.sqrt((p * (1 - p)) / totalCount)\n",
    "\t\n",
    "\t# Calculate the Z-score for the desired confidence level\n",
    "\tz = stats.norm.ppf(1 - (1 - confidenceLevel) / 2)\n",
    "\t\n",
    "\t# Calculate the margin of error\n",
    "\tmargin_of_error = z * se\n",
    "\t\n",
    "\t# Calculate the lower and upper bounds of the confidence interval\n",
    "\tlower_bound = p - margin_of_error\n",
    "\tupper_bound = p + margin_of_error\n",
    "\t\n",
    "\treturn lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalStandardError(positives, totalCount):\n",
    "\tp = positives / totalCount\n",
    "\treturn math.sqrt((p * (1 - p)) / totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake - Human\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Norm_Percent</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgentType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HardCoded</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.048734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML</th>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.056458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted     Percent Norm_Percent     Error\n",
       "              count sum                               \n",
       "AgentType                                             \n",
       "HardCoded        20   1    0.05     0.066667  0.048734\n",
       "Human            12   9    0.75     1.000000  0.125000\n",
       "ML               40  34    0.85     1.133333  0.056458\n",
       "Random           20   0    0.00     0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake - Curated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\AppData\\Local\\Temp\\ipykernel_23480\\2844687134.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  p = positives / totalCount\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Norm_Percent</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgentType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HardCoded</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted      Percent Norm_Percent Error\n",
       "              count  sum                           \n",
       "AgentType                                          \n",
       "HardCoded         0  0.0     NaN          NaN   NaN\n",
       "Human             0  0.0     NaN          NaN   NaN\n",
       "ML                0  0.0     NaN          NaN   NaN\n",
       "Random            0  0.0     NaN          NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LoadReplaysToReview(envName, runGroup, behaviourType):\n",
    "\treplaysToReviewPath = os.path.join(\"Data\", envName, runGroup, f\"ReplaysToReview_{behaviourType}.json\")\n",
    "\treplaysToReview = pd.read_json(replaysToReviewPath)\n",
    "\n",
    "\tgrouped = replaysToReview.groupby(\"AgentType\").aggregate({\"Predicted\": [\"count\", \"sum\"]})\n",
    "\n",
    "\n",
    "\tgrouped[\"Percent\"] = grouped[\"Predicted\"][\"sum\"] / grouped[\"Predicted\"][\"count\"]\n",
    "\tgrouped[\"Norm_Percent\"] = grouped[\"Percent\"] / grouped[\"Percent\"][\"Human\"]\n",
    "\n",
    "\t# calculate the confidence intervals\n",
    "\tgrouped[\"Error\"] = grouped.apply(lambda x: CalStandardError(x[\"Predicted\"][\"sum\"], x[\"Predicted\"][\"count\"]), axis=1)\n",
    "\treturn grouped\n",
    "\n",
    "for envName in EnvNames:\n",
    "\tfor behaviourType in BehaviouralTypesToReview:\n",
    "\t\tgrouped = LoadReplaysToReview(envName, RunGroup, behaviourType)\n",
    "\t\tprint(f\"{envName} - {behaviourType}\")\n",
    "\t\tdisplay(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating tables and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RunGroup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m \t\t\t\t\tdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, pd\u001b[39m.\u001b[39mDataFrame(row)], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \t\u001b[39mreturn\u001b[39;00m df\n\u001b[1;32m---> 28\u001b[0m evalIds \u001b[39m=\u001b[39m CollectEvalIds(RunGroup, EnvNames, BehaviouralTypes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RunGroup' is not defined"
     ]
    }
   ],
   "source": [
    "def CollectEvalIds(runGroup, envNames, behaviouralTypes):\n",
    "\tcolumns = [\"EnvName\", \"AgentId\", \"AgentType\", \"EpisodeId\", \"Behaviour\"]\n",
    "\tdf = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\tfor behaviourType in behaviouralTypes:\n",
    "\t\t\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\t\t\treplays =  ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\n",
    "\n",
    "\t\t\tfor agentId, episodeIds in replays.items():\n",
    "\t\t\t\t\n",
    "\t\t\t\tagentType = agentId.split(\"_\")[0]\n",
    "\n",
    "\t\t\t\tfor i in range(len(episodeIds)):\n",
    "\t\t\t\t\tepisodeId = episodeIds[i]\n",
    "\n",
    "\t\t\t\t\trow = {}\n",
    "\t\t\t\t\trow[\"EnvName\"] = [envName]\n",
    "\t\t\t\t\trow[\"AgentId\"] = [agentId]\n",
    "\t\t\t\t\trow[\"AgentType\"] = [agentType]\n",
    "\t\t\t\t\trow[\"Behaviour\"] = [behaviourType]\n",
    "\t\t\t\t\trow[\"EpisodeId\"] = [episodeId]\n",
    "\t\t\t\t\tdf = pd.concat([df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "evalIds = CollectEvalIds(RunGroup, EnvNames, BehaviouralTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanStats(df):\n",
    "\tprefixesToDrop = [\"LearnerConfig\", \"ModelConfigs\", \"DataTables\"]\n",
    "\tcolumnsToDrop = [col for col in df.columns if col.startswith(tuple(prefixesToDrop))]\n",
    "\tdf = df.drop(columns=columnsToDrop)\n",
    "\n",
    "\t# add duration column\n",
    "\tdf[\"Duration\"] = (df[\"EndTime\"] - df[\"StartTime\"])  / 1e9\n",
    "\treturn df\n",
    "\n",
    "def CombinedStats(runGroup, envNames, agentTypes):\n",
    "\tcombinedStats = None\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\tfor agentType in agentTypes:\n",
    "\t\t\tstatsPath = os.path.join(\"Data\", envName, runGroup, \"replays\", agentType, \"stats.tsv\")\n",
    "\t\t\tstats = pd.read_csv(statsPath, sep=\"\\t\")\n",
    "\t\t\tstats = CleanStats(stats)\n",
    "\n",
    "\t\t\tcombinedStats = pd.concat([combinedStats, stats], ignore_index=True)\n",
    "\treturn combinedStats\n",
    "\n",
    "stats = CombinedStats(RunGroup, EnvNames, AgentTypes)\n",
    "evalDf = pd.merge(evalIds, stats, on=[\"EpisodeId\"], how=\"left\")\n",
    "\n",
    "# drop rows with nan values\n",
    "evalDf = evalDf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertToLatex(df):\n",
    "\t# Get column names\n",
    "\tcolumns = df.columns.tolist()\n",
    "\n",
    "\theaderCode = \"\\hline\\n\"\n",
    "\theaderCode += \"\\t\\multicolumn{1}{|c|}{\\\\textbf{\"\n",
    "\theaderCode += \"}} &\\n\\t\\multicolumn{1}{c|}{\\\\textbf{\".join(columns)\n",
    "\n",
    "\theaderCode += \"}} \\\\\\\\\\n\\hline\\n\"\n",
    "\n",
    "\n",
    "\t# Generate LaTeX table code\n",
    "\tlatex_code = \"\\\\begin{longtable}{|\" + \"c|\" * len(columns) + \"}\\n\"\n",
    "\n",
    "\t# add caption and label\n",
    "\tlatex_code += \"\\\\caption{Insert Caption Here.}\\n\"\n",
    "\tlatex_code += \"\\\\label{tab:InsertLabelHere} \\\\\\\\\\n\"\n",
    "\n",
    "\tlatex_code += headerCode\n",
    "\tlatex_code += \"\\endfirsthead\\n\\n\"\n",
    "\n",
    "\tlatex_code += \"\\multicolumn{\" + str(len(columns)) + \"}{c}%\\n\"\n",
    "\tlatex_code += \"{{\\\\bfseries \\\\tablename\\\\ \\\\thetable{} -- continued from previous page}} \\\\\\\\\\n\"\n",
    "\tlatex_code += headerCode\n",
    "\tlatex_code += \"\\endhead\\n\\n\"\n",
    "\n",
    "\tlatex_code += \"\\hline \\multicolumn{\" + str(len(columns)) + \"}{|c|}{{Continued on next page}} \\\\\\\\ \\hline\\n\\n\"\n",
    "\tlatex_code += \"\\endfoot\\n\"\n",
    "\n",
    "\tlatex_code += \"\\hline\\n\"\n",
    "\tlatex_code += \"\\endlastfoot\\n\"\n",
    "\n",
    "\tlatex_code += \"\\n\"\n",
    "\n",
    "\n",
    "\t# Add data rows\n",
    "\tfor index, row in df.iterrows():\n",
    "\t\tvalues = row.tolist()\n",
    "\t\tlatex_code += \"\\t\" + \" & \".join(str(value) for value in values) + \" \\\\\\\\\\n\"\n",
    "\tlatex_code += \"\\\\hline\\n\"\n",
    "\t# Complete LaTeX table code\n",
    "\tlatex_code += \"\\\\end{longtable}\"\n",
    "\n",
    "\treturn latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>EpisodeId</th>\n",
       "      <th>Duration</th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalReward</th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalCuratedReward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th>Behaviour</th>\n",
       "      <th>AgentId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">FrozenLake</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">Curated</th>\n",
       "      <th>HardCoded_D_1_RT_True_Curated</th>\n",
       "      <td>100</td>\n",
       "      <td>0.039966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_Curated</th>\n",
       "      <td>100</td>\n",
       "      <td>0.340874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_Curated</th>\n",
       "      <td>100</td>\n",
       "      <td>0.169350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_Curated</th>\n",
       "      <td>112</td>\n",
       "      <td>2.317554</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.207447</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.286437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">HighScore</th>\n",
       "      <th>HardCoded_D_1_RT_True_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.380918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.138895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_HighScore</th>\n",
       "      <td>1008</td>\n",
       "      <td>2.385675</td>\n",
       "      <td>0.038690</td>\n",
       "      <td>0.192952</td>\n",
       "      <td>0.124008</td>\n",
       "      <td>0.329754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Human</th>\n",
       "      <th>HardCoded_D_1_RT_True_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.014433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.884688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.345229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_Human</th>\n",
       "      <td>116</td>\n",
       "      <td>1.755059</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.159412</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.305865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     EpisodeId  Duration  \\\n",
       "                                                         count      mean   \n",
       "EnvName    Behaviour AgentId                                               \n",
       "FrozenLake Curated   HardCoded_D_1_RT_True_Curated         100  0.039966   \n",
       "                     ML_D_10_RT_False_Curated              100  0.340874   \n",
       "                     ML_D_10_RT_True_Curated               100  0.169350   \n",
       "                     Random_D_1_RT_True_Curated            112  2.317554   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore      1000  0.020397   \n",
       "                     ML_D_10_RT_False_HighScore           1000  0.380918   \n",
       "                     ML_D_10_RT_True_HighScore            1000  0.138895   \n",
       "                     Random_D_1_RT_True_HighScore         1008  2.385675   \n",
       "           Human     HardCoded_D_1_RT_True_Human           100  0.014433   \n",
       "                     ML_D_10_RT_False_Human                100  0.884688   \n",
       "                     ML_D_10_RT_True_Human                 100  0.345229   \n",
       "                     Random_D_1_RT_True_Human              116  1.755059   \n",
       "\n",
       "                                                     EpisodeTotalReward  \\\n",
       "                                                                   mean   \n",
       "EnvName    Behaviour AgentId                                              \n",
       "FrozenLake Curated   HardCoded_D_1_RT_True_Curated             1.000000   \n",
       "                     ML_D_10_RT_False_Curated                  1.000000   \n",
       "                     ML_D_10_RT_True_Curated                   1.000000   \n",
       "                     Random_D_1_RT_True_Curated                0.044643   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore           1.000000   \n",
       "                     ML_D_10_RT_False_HighScore                1.000000   \n",
       "                     ML_D_10_RT_True_HighScore                 1.000000   \n",
       "                     Random_D_1_RT_True_HighScore              0.038690   \n",
       "           Human     HardCoded_D_1_RT_True_Human               1.000000   \n",
       "                     ML_D_10_RT_False_Human                    1.000000   \n",
       "                     ML_D_10_RT_True_Human                     1.000000   \n",
       "                     Random_D_1_RT_True_Human                  0.025862   \n",
       "\n",
       "                                                                \\\n",
       "                                                           std   \n",
       "EnvName    Behaviour AgentId                                     \n",
       "FrozenLake Curated   HardCoded_D_1_RT_True_Curated    0.000000   \n",
       "                     ML_D_10_RT_False_Curated         0.000000   \n",
       "                     ML_D_10_RT_True_Curated          0.000000   \n",
       "                     Random_D_1_RT_True_Curated       0.207447   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore  0.000000   \n",
       "                     ML_D_10_RT_False_HighScore       0.000000   \n",
       "                     ML_D_10_RT_True_HighScore        0.000000   \n",
       "                     Random_D_1_RT_True_HighScore     0.192952   \n",
       "           Human     HardCoded_D_1_RT_True_Human      0.000000   \n",
       "                     ML_D_10_RT_False_Human           0.000000   \n",
       "                     ML_D_10_RT_True_Human            0.000000   \n",
       "                     Random_D_1_RT_True_Human         0.159412   \n",
       "\n",
       "                                                     EpisodeTotalCuratedReward  \\\n",
       "                                                                          mean   \n",
       "EnvName    Behaviour AgentId                                                     \n",
       "FrozenLake Curated   HardCoded_D_1_RT_True_Curated                    1.000000   \n",
       "                     ML_D_10_RT_False_Curated                         0.000000   \n",
       "                     ML_D_10_RT_True_Curated                          0.000000   \n",
       "                     Random_D_1_RT_True_Curated                       0.089286   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore                  0.000000   \n",
       "                     ML_D_10_RT_False_HighScore                       0.000000   \n",
       "                     ML_D_10_RT_True_HighScore                        0.000000   \n",
       "                     Random_D_1_RT_True_HighScore                     0.124008   \n",
       "           Human     HardCoded_D_1_RT_True_Human                      0.000000   \n",
       "                     ML_D_10_RT_False_Human                           0.000000   \n",
       "                     ML_D_10_RT_True_Human                            0.000000   \n",
       "                     Random_D_1_RT_True_Human                         0.103448   \n",
       "\n",
       "                                                                \n",
       "                                                           std  \n",
       "EnvName    Behaviour AgentId                                    \n",
       "FrozenLake Curated   HardCoded_D_1_RT_True_Curated    0.000000  \n",
       "                     ML_D_10_RT_False_Curated         0.000000  \n",
       "                     ML_D_10_RT_True_Curated          0.000000  \n",
       "                     Random_D_1_RT_True_Curated       0.286437  \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore  0.000000  \n",
       "                     ML_D_10_RT_False_HighScore       0.000000  \n",
       "                     ML_D_10_RT_True_HighScore        0.000000  \n",
       "                     Random_D_1_RT_True_HighScore     0.329754  \n",
       "           Human     HardCoded_D_1_RT_True_Human      0.000000  \n",
       "                     ML_D_10_RT_False_Human           0.000000  \n",
       "                     ML_D_10_RT_True_Human            0.000000  \n",
       "                     Random_D_1_RT_True_Human         0.305865  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregateSettings = {}\n",
    "aggregateSettings[\"EpisodeId\"] = \"count\"\n",
    "aggregateSettings[\"Duration\"] = [\"mean\"]\n",
    "aggregateSettings[\"EpisodeTotalReward\"] = [\"mean\", \"std\"]\n",
    "aggregateSettings[\"EpisodeTotalCuratedReward\"] = [\"mean\", \"std\"]\n",
    "\n",
    "evalDf.groupby([\"EnvName\", \"Behaviour\", \"AgentId\"]).aggregate(aggregateSettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalReward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th>AgentId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">FrozenLake</th>\n",
       "      <th>HardCoded_D_1_RT_True_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_HighScore</th>\n",
       "      <td>0.03869</td>\n",
       "      <td>0.192952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           EpisodeTotalReward          \n",
       "                                                         mean       std\n",
       "EnvName    AgentId                                                     \n",
       "FrozenLake HardCoded_D_1_RT_True_HighScore            1.00000  0.000000\n",
       "           ML_D_10_RT_False_HighScore                 1.00000  0.000000\n",
       "           ML_D_10_RT_True_HighScore                  1.00000  0.000000\n",
       "           Random_D_1_RT_True_HighScore               0.03869  0.192952"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "evalDf[evalDf[\"Behaviour\"] == \"HighScore\"].groupby([\"EnvName\", \"AgentId\"])[[\"EpisodeTotalReward\"]].aggregate(aggregateSettings[\"EpisodeTotalReward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEnvAgentTypeTable(evalDf, envNames, agentTypes, metric):\n",
    "\n",
    "\tcolumns = [\"EnvName\"]\n",
    "\tcolumns += agentTypes\n",
    "\n",
    "\tdf = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\trow = {}\n",
    "\t\trow[\"EnvName\"] = [envName]\n",
    "\n",
    "\t\tfor agentType in agentTypes:\n",
    "\t\t\tagentTypeDf = evalDf[evalDf[\"AgentType\"] == agentType]\n",
    "\t\t\tagentTypeDf = agentTypeDf[agentTypeDf[\"EnvName\"] == envName]\n",
    "\n",
    "\t\t\tavg = agentTypeDf[metric].mean()\n",
    "\t\t\terror = agentTypeDf[metric].std()\n",
    "\t\t\tcell = f\"{avg:.2f}\"# ±{error:.2f}\"\n",
    "\t\t\trow[agentType] = [cell]\n",
    "\n",
    "\t\tdf = pd.concat([df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "\t\n",
    "\t# set the index to be the env name\n",
    "\tdf = df.set_index(\"EnvName\")\n",
    "\n",
    "\ttext = ConvertToLatex(df)\n",
    "\tpc.copy(text)\n",
    "\tprint(text.replace(\"\\n\", \" \"))\n",
    "\tdisplay(df)\n",
    "\tprint(\"Copied to clipboard\")\n",
    "\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evalDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m curatedDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCurated\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m humanDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHuman\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m highScoreDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHighScore\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evalDf' is not defined"
     ]
    }
   ],
   "source": [
    "curatedDf = evalDf[evalDf[\"Behaviour\"] == \"Curated\"]\n",
    "humanDf = evalDf[evalDf[\"Behaviour\"] == \"Human\"]\n",
    "highScoreDf = evalDf[evalDf[\"Behaviour\"] == \"HighScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t1.00 ±0.00 & 1.00 ±0.00 & 0.04 ±0.19 & nan ±nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>0.04 ±0.19</td>\n",
       "      <td>nan ±nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HardCoded          ML      Random     Human\n",
       "EnvName                                                 \n",
       "FrozenLake  1.00 ±0.00  1.00 ±0.00  0.04 ±0.19  nan ±nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(highScoreDf, EnvNames, AgentTypes, \"EpisodeTotalReward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t1.00 ±0.00 & 0.00 ±0.00 & 0.09 ±0.29 & nan ±nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>0.00 ±0.00</td>\n",
       "      <td>0.09 ±0.29</td>\n",
       "      <td>nan ±nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HardCoded          ML      Random     Human\n",
       "EnvName                                                 \n",
       "FrozenLake  1.00 ±0.00  0.00 ±0.00  0.09 ±0.29  nan ±nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(curatedDf, EnvNames, AgentTypes, \"EpisodeTotalCuratedReward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t0.02 ±0.00 & 0.26 ±2.74 & 2.39 ±1.41 & nan ±nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>0.02 ±0.00</td>\n",
       "      <td>0.26 ±2.74</td>\n",
       "      <td>2.39 ±1.41</td>\n",
       "      <td>nan ±nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HardCoded          ML      Random     Human\n",
       "EnvName                                                 \n",
       "FrozenLake  0.02 ±0.00  0.26 ±2.74  2.39 ±1.41  nan ±nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(highScoreDf, EnvNames, AgentTypes, \"Duration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
