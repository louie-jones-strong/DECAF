{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Formatting\n",
    "\n",
    "This notebook was used for collecting and formatting the results of the experiments, for the evaluation section of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import src.Common.Utils.Config.ConfigHelper as ConfigHelper\n",
    "import shutil\n",
    "import src.Common.EpisodeReplay.EpisodeReplay as EpisodeReplay\n",
    "from tqdm import tqdm\n",
    "import pyperclip as pc\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunGroup = \"13\"\n",
    "EnvNames = [\"FrozenLake\", \"CartPole\"]\n",
    "BehaviouralTypes = [\"Human\", \"Curated\", \"HighScore\"]\n",
    "BehaviouralTypesToReview = [\"Human\", \"Curated\"]\n",
    "AgentTypes = [\"HardCoded\", \"ML\", \"Random\", \"Human\"]\n",
    "\n",
    "# manual Review of the results config\n",
    "MaxChoicesPerAgent = 5\n",
    "MaxReplaysPerChoice = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied Human Demos for: FrozenLake\n",
      "Added 59 Human episodes to FrozenLake\n",
      "Added 23 Curated episodes to FrozenLake\n",
      "Added 82 HighScore episodes to FrozenLake\n"
     ]
    }
   ],
   "source": [
    "# copy demos of each behavioural type to the run group folder\n",
    "def CopyDemos(envName, runGroup):\n",
    "\tfromPath = os.path.join(\"Data\", envName, \"dev\", \"replays\", \"Human\")\n",
    "\ttoPath = os.path.join(\"Data\", envName, runGroup, \"replays\", \"Human\")\n",
    "\n",
    "\tif os.path.exists(toPath):\n",
    "\t\tshutil.rmtree(toPath)\n",
    "\tshutil.copytree(fromPath, toPath)\n",
    "\n",
    "\tprint(\"Copied Human Demos for: \" + envName)\n",
    "\treturn\n",
    "\n",
    "def AddDemoIdsToBehaviour(envName, runGroup, behaviourType):\n",
    "\t# load the results\n",
    "\tstatsPath = os.path.join(\"Data\", envName, runGroup, \"replays\", \"Human\", \"stats.tsv\")\n",
    "\tstats = pd.read_csv(statsPath, sep=\"\\t\")\n",
    "\n",
    "\tlowerBehaviourType = behaviourType.lower()\n",
    "\n",
    "\tstats[\"Behaviour\"] = stats[\"metricName\"].apply(lambda x: x.split(\"_\")[-2])\n",
    "\n",
    "\tif lowerBehaviourType != \"highscore\":\n",
    "\t\tstats = stats[stats[\"Behaviour\"] == lowerBehaviourType]\n",
    "\n",
    "\tepisodeIds = stats[\"EpisodeId\"].unique().tolist()\n",
    "\n",
    "\tmetricName = \"Human_\" + behaviourType\n",
    "\n",
    "\t# load the json with the episode Ids of the behavioural type\n",
    "\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\t\n",
    "\tepisodeIdsJson = ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\tepisodeIdsJson[metricName] = episodeIds\n",
    "\n",
    "\tConfigHelper.SaveConfig(episodeIdsJson, episodeIdsPath)\n",
    "\n",
    "\tprint(f\"Added {len(episodeIds)} {behaviourType} episodes to {envName}\")\n",
    "\treturn\n",
    "\n",
    "for envName in EnvNames:\n",
    "\tCopyDemos(envName, RunGroup)\n",
    "\n",
    "\tfor behaviourType in BehaviouralTypes:\n",
    "\t\tAddDemoIdsToBehaviour(envName, RunGroup, behaviourType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Replays For Manual Reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 92 replays to review for Human in FrozenLake\n",
      "Collected 39 replays to review for Curated in FrozenLake\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "configPath Data\\CartPole\\13\\Human_Episodes.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\ResultsFormating.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m envName \u001b[39min\u001b[39;00m EnvNames:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m behaviourType \u001b[39min\u001b[39;00m BehaviouralTypesToReview:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \t\treplaysToReview \u001b[39m=\u001b[39m CollectReplaysToReview(envName, RunGroup, behaviourType)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \t\treplaysToReview \u001b[39m=\u001b[39m replaysToReview\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \t\treplaysToReviewPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mData\u001b[39m\u001b[39m\"\u001b[39m, envName, RunGroup, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReplaysToReview_\u001b[39m\u001b[39m{\u001b[39;00mbehaviourType\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\ResultsFormating.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mCollectReplaysToReview\u001b[39m(envName, runGroup, behaviourType):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \tepisodeIdsPath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mData\u001b[39m\u001b[39m\"\u001b[39m, envName, runGroup, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbehaviourType\u001b[39m}\u001b[39;00m\u001b[39m_Episodes.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \treplays \u001b[39m=\u001b[39m  ConfigHelper\u001b[39m.\u001b[39;49mLoadConfig(episodeIdsPath)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \tcolumns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mAgentId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAgentType\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \tcolumns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReplay_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MaxReplaysPerChoice)]\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\src\\Common\\Utils\\Config\\ConfigHelper.py:35\u001b[0m, in \u001b[0;36mLoadConfig\u001b[1;34m(configPath)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadConfig\u001b[39m(configPath:\u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SCT\u001b[39m.\u001b[39mConfig:\n\u001b[0;32m     34\u001b[0m \t\u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(configPath):\n\u001b[1;32m---> 35\u001b[0m \t\t\u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigPath \u001b[39m\u001b[39m{\u001b[39;00mconfigPath\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \t\u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(configPath):\n\u001b[0;32m     38\u001b[0m \t\t\u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigPath \u001b[39m\u001b[39m{\u001b[39;00mconfigPath\u001b[39m}\u001b[39;00m\u001b[39m is not a file\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: configPath Data\\CartPole\\13\\Human_Episodes.json does not exist"
     ]
    }
   ],
   "source": [
    "def LoadReplay(envName, runGroup, agentType, episodeId):\n",
    "\t\n",
    "\tpath = os.path.join(\"Data\", envName, runGroup, \"replays\", agentType, episodeId)\n",
    "\ttry:\n",
    "\t\treplay = EpisodeReplay.EpisodeReplay.LoadFromFolder(path)\n",
    "\t\treturn replay\n",
    "\texcept:\n",
    "\t\treturn None\n",
    "\n",
    "def CollectReplaysToReview(envName, runGroup, behaviourType):\n",
    "\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\treplays =  ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\n",
    "\tcolumns = [\"AgentId\", \"Predicted\", \"AgentType\"]\n",
    "\tcolumns += [f\"Replay_{i}\" for i in range(MaxReplaysPerChoice)]\n",
    "\n",
    "\treplaysToReview = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor agentId, episodeIds in replays.items():\n",
    "\t\t\n",
    "\t\tagentType = agentId.split(\"_\")[0]\n",
    "\t\tids = []\n",
    "\n",
    "\t\tfor i in range(len(episodeIds)):\n",
    "\t\t\tepisodeId = episodeIds[i]\n",
    "\n",
    "\t\t\treplay = LoadReplay(envName, runGroup, agentType, episodeId)\n",
    "\t\t\tif replay is None:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tids.append(episodeId)\n",
    "\n",
    "\t\t\tif len(ids) >= MaxReplaysPerChoice or i == len(episodeIds) - 1:\n",
    "\t\t\t\trow = {}\n",
    "\t\t\t\trow[\"AgentId\"] = [agentId]\n",
    "\t\t\t\trow[\"Predicted\"] = [None]\n",
    "\t\t\t\trow[\"AgentType\"] = [agentType]\n",
    "\t\t\t\tfor i, id in enumerate(ids):\n",
    "\t\t\t\t\trow[f\"Replay_{i}\"] = [id]\n",
    "\n",
    "\t\t\t\treplaysToReview = pd.concat([replaysToReview, pd.DataFrame(row)], ignore_index=True)\n",
    "\t\t\t\tids = []\n",
    "\n",
    "\treturn replaysToReview\n",
    "\t\n",
    "for envName in EnvNames:\n",
    "\tfor behaviourType in BehaviouralTypesToReview:\n",
    "\n",
    "\t\treplaysToReview = CollectReplaysToReview(envName, RunGroup, behaviourType)\n",
    "\t\treplaysToReview = replaysToReview.sample(frac=1)\n",
    "\t\treplaysToReviewPath = os.path.join(\"Data\", envName, RunGroup, f\"ReplaysToReview_{behaviourType}.json\")\n",
    "\n",
    "\t\treplaysToReview.to_json(replaysToReviewPath, orient=\"records\", indent=4)\n",
    "\n",
    "\t\tprint(f\"Collected {len(replaysToReview)} replays to review for {behaviourType} in {envName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formate the results of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateBinaryCI(positives, totalCount, confidenceLevel=0.95):\n",
    "\t# Calculate the sample proportion (p)\n",
    "\tp = positives / totalCount\n",
    "\t\n",
    "\t# Calculate the standard error\n",
    "\tse = math.sqrt((p * (1 - p)) / totalCount)\n",
    "\t\n",
    "\t# Calculate the Z-score for the desired confidence level\n",
    "\tz = stats.norm.ppf(1 - (1 - confidenceLevel) / 2)\n",
    "\t\n",
    "\t# Calculate the margin of error\n",
    "\tmargin_of_error = z * se\n",
    "\t\n",
    "\t# Calculate the lower and upper bounds of the confidence interval\n",
    "\tlower_bound = p - margin_of_error\n",
    "\tupper_bound = p + margin_of_error\n",
    "\t\n",
    "\treturn lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalStandardError(positives, totalCount):\n",
    "\tp = positives / totalCount\n",
    "\treturn math.sqrt((p * (1 - p)) / totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake - Human\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Norm_Percent</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgentType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HardCoded</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.048734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML</th>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.056458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted     Percent Norm_Percent     Error\n",
       "              count sum                               \n",
       "AgentType                                             \n",
       "HardCoded        20   1    0.05     0.066667  0.048734\n",
       "Human            12   9    0.75     1.000000  0.125000\n",
       "ML               40  34    0.85     1.133333  0.056458\n",
       "Random           20   0    0.00     0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake - Curated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\AppData\\Local\\Temp\\ipykernel_23480\\2844687134.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  p = positives / totalCount\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Norm_Percent</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AgentType</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HardCoded</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted      Percent Norm_Percent Error\n",
       "              count  sum                           \n",
       "AgentType                                          \n",
       "HardCoded         0  0.0     NaN          NaN   NaN\n",
       "Human             0  0.0     NaN          NaN   NaN\n",
       "ML                0  0.0     NaN          NaN   NaN\n",
       "Random            0  0.0     NaN          NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LoadReplaysToReview(envName, runGroup, behaviourType):\n",
    "\treplaysToReviewPath = os.path.join(\"Data\", envName, runGroup, f\"ReplaysToReview_{behaviourType}.json\")\n",
    "\treplaysToReview = pd.read_json(replaysToReviewPath)\n",
    "\n",
    "\tgrouped = replaysToReview.groupby(\"AgentType\").aggregate({\"Predicted\": [\"count\", \"sum\"]})\n",
    "\n",
    "\n",
    "\tgrouped[\"Percent\"] = grouped[\"Predicted\"][\"sum\"] / grouped[\"Predicted\"][\"count\"]\n",
    "\tgrouped[\"Norm_Percent\"] = grouped[\"Percent\"] / grouped[\"Percent\"][\"Human\"]\n",
    "\n",
    "\t# calculate the confidence intervals\n",
    "\tgrouped[\"Error\"] = grouped.apply(lambda x: CalStandardError(x[\"Predicted\"][\"sum\"], x[\"Predicted\"][\"count\"]), axis=1)\n",
    "\treturn grouped\n",
    "\n",
    "for envName in EnvNames:\n",
    "\tfor behaviourType in BehaviouralTypesToReview:\n",
    "\t\tgrouped = LoadReplaysToReview(envName, RunGroup, behaviourType)\n",
    "\t\tprint(f\"{envName} - {behaviourType}\")\n",
    "\t\tdisplay(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% (8.94%)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "positiveCount = 16\n",
    "totalCount = 20\n",
    "\n",
    "\n",
    "p = positiveCount / totalCount\n",
    "error = math.sqrt((p * (1 - p)) / totalCount)\n",
    "\n",
    "p *= 100\n",
    "error *= 100\n",
    "print(f\"{p:.3g}% ({error:.3g}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating tables and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CollectEvalIds(runGroup, envNames, behaviouralTypes):\n",
    "\tcolumns = [\"EnvName\", \"AgentId\", \"AgentType\", \"EpisodeId\", \"Behaviour\"]\n",
    "\tdf = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\tfor behaviourType in behaviouralTypes:\n",
    "\t\t\tepisodeIdsPath = os.path.join(\"Data\", envName, runGroup, f\"{behaviourType}_Episodes.json\")\n",
    "\t\t\treplays =  ConfigHelper.LoadConfig(episodeIdsPath)\n",
    "\n",
    "\n",
    "\t\t\tfor agentId, episodeIds in replays.items():\n",
    "\t\t\t\t\n",
    "\t\t\t\tagentType = agentId.split(\"_\")[0]\n",
    "\n",
    "\t\t\t\tfor i in range(len(episodeIds)):\n",
    "\t\t\t\t\tepisodeId = episodeIds[i]\n",
    "\n",
    "\t\t\t\t\trow = {}\n",
    "\t\t\t\t\trow[\"EnvName\"] = [envName]\n",
    "\t\t\t\t\trow[\"AgentId\"] = [agentId]\n",
    "\t\t\t\t\trow[\"AgentType\"] = [agentType]\n",
    "\t\t\t\t\trow[\"Behaviour\"] = [behaviourType]\n",
    "\t\t\t\t\trow[\"EpisodeId\"] = [episodeId]\n",
    "\t\t\t\t\tdf = pd.concat([df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "evalIds = CollectEvalIds(RunGroup, EnvNames, BehaviouralTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanStats(df):\n",
    "\tprefixesToDrop = [\"LearnerConfig\", \"ModelConfigs\", \"DataTables\"]\n",
    "\tcolumnsToDrop = [col for col in df.columns if col.startswith(tuple(prefixesToDrop))]\n",
    "\tdf = df.drop(columns=columnsToDrop)\n",
    "\n",
    "\t# add duration column\n",
    "\tdf[\"Duration\"] = (df[\"EndTime\"] - df[\"StartTime\"])  / 1e9\n",
    "\treturn df\n",
    "\n",
    "def CombinedStats(runGroup, envNames, agentTypes):\n",
    "\tcombinedStats = None\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\tfor agentType in agentTypes:\n",
    "\t\t\tstatsPath = os.path.join(\"Data\", envName, runGroup, \"replays\", agentType, \"stats.tsv\")\n",
    "\t\t\tstats = pd.read_csv(statsPath, sep=\"\\t\")\n",
    "\t\t\tstats = CleanStats(stats)\n",
    "\n",
    "\t\t\tcombinedStats = pd.concat([combinedStats, stats], ignore_index=True)\n",
    "\treturn combinedStats\n",
    "\n",
    "stats = CombinedStats(RunGroup, EnvNames, AgentTypes)\n",
    "evalDf = pd.merge(evalIds, stats, on=[\"EpisodeId\"], how=\"left\")\n",
    "\n",
    "# drop rows with nan values\n",
    "evalDf = evalDf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertToLatex(df):\n",
    "\t# Get column names\n",
    "\tcolumns = df.columns.tolist()\n",
    "\n",
    "\theaderCode = \"\\hline\\n\"\n",
    "\theaderCode += \"\\t\\multicolumn{1}{|c|}{\\\\textbf{\"\n",
    "\theaderCode += \"}} &\\n\\t\\multicolumn{1}{c|}{\\\\textbf{\".join(columns)\n",
    "\n",
    "\theaderCode += \"}} \\\\\\\\\\n\\hline\\n\"\n",
    "\n",
    "\n",
    "\t# Generate LaTeX table code\n",
    "\tlatex_code = \"\\\\begin{longtable}{|\" + \"c|\" * len(columns) + \"}\\n\"\n",
    "\n",
    "\t# add caption and label\n",
    "\tlatex_code += \"\\\\caption{Insert Caption Here.}\\n\"\n",
    "\tlatex_code += \"\\\\label{tab:InsertLabelHere} \\\\\\\\\\n\"\n",
    "\n",
    "\tlatex_code += headerCode\n",
    "\tlatex_code += \"\\endfirsthead\\n\\n\"\n",
    "\n",
    "\tlatex_code += \"\\multicolumn{\" + str(len(columns)) + \"}{c}%\\n\"\n",
    "\tlatex_code += \"{{\\\\bfseries \\\\tablename\\\\ \\\\thetable{} -- continued from previous page}} \\\\\\\\\\n\"\n",
    "\tlatex_code += headerCode\n",
    "\tlatex_code += \"\\endhead\\n\\n\"\n",
    "\n",
    "\tlatex_code += \"\\hline \\multicolumn{\" + str(len(columns)) + \"}{|c|}{{Continued on next page}} \\\\\\\\ \\hline\\n\\n\"\n",
    "\tlatex_code += \"\\endfoot\\n\"\n",
    "\n",
    "\tlatex_code += \"\\hline\\n\"\n",
    "\tlatex_code += \"\\endlastfoot\\n\"\n",
    "\n",
    "\tlatex_code += \"\\n\"\n",
    "\n",
    "\n",
    "\t# Add data rows\n",
    "\tfor index, row in df.iterrows():\n",
    "\t\tvalues = row.tolist()\n",
    "\t\tlatex_code += \"\\t\" + \" & \".join(str(value) for value in values) + \" \\\\\\\\\\n\"\n",
    "\tlatex_code += \"\\\\hline\\n\"\n",
    "\t# Complete LaTeX table code\n",
    "\tlatex_code += \"\\\\end{longtable}\"\n",
    "\n",
    "\treturn latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>EpisodeId</th>\n",
       "      <th>Duration</th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalReward</th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalCuratedReward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th>Behaviour</th>\n",
       "      <th>AgentId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">FrozenLake</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Curated</th>\n",
       "      <th>ML_D_10_RT_False_Curated</th>\n",
       "      <td>100</td>\n",
       "      <td>0.424349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_Curated</th>\n",
       "      <td>100</td>\n",
       "      <td>0.250854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">HighScore</th>\n",
       "      <th>HardCoded_D_1_RT_True_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.380918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_HighScore</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.138895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_HighScore</th>\n",
       "      <td>1008</td>\n",
       "      <td>2.385675</td>\n",
       "      <td>0.038690</td>\n",
       "      <td>0.192952</td>\n",
       "      <td>0.124008</td>\n",
       "      <td>0.329754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Human</th>\n",
       "      <th>HardCoded_D_1_RT_True_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.014433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.884688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_Human</th>\n",
       "      <td>100</td>\n",
       "      <td>0.345229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_Human</th>\n",
       "      <td>116</td>\n",
       "      <td>1.755059</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.159412</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.305865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     EpisodeId  Duration  \\\n",
       "                                                         count      mean   \n",
       "EnvName    Behaviour AgentId                                               \n",
       "FrozenLake Curated   ML_D_10_RT_False_Curated              100  0.424349   \n",
       "                     ML_D_10_RT_True_Curated               100  0.250854   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore      1000  0.020397   \n",
       "                     ML_D_10_RT_False_HighScore           1000  0.380918   \n",
       "                     ML_D_10_RT_True_HighScore            1000  0.138895   \n",
       "                     Random_D_1_RT_True_HighScore         1008  2.385675   \n",
       "           Human     HardCoded_D_1_RT_True_Human           100  0.014433   \n",
       "                     ML_D_10_RT_False_Human                100  0.884688   \n",
       "                     ML_D_10_RT_True_Human                 100  0.345229   \n",
       "                     Random_D_1_RT_True_Human              116  1.755059   \n",
       "\n",
       "                                                     EpisodeTotalReward  \\\n",
       "                                                                   mean   \n",
       "EnvName    Behaviour AgentId                                              \n",
       "FrozenLake Curated   ML_D_10_RT_False_Curated                  1.000000   \n",
       "                     ML_D_10_RT_True_Curated                   1.000000   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore           1.000000   \n",
       "                     ML_D_10_RT_False_HighScore                1.000000   \n",
       "                     ML_D_10_RT_True_HighScore                 1.000000   \n",
       "                     Random_D_1_RT_True_HighScore              0.038690   \n",
       "           Human     HardCoded_D_1_RT_True_Human               1.000000   \n",
       "                     ML_D_10_RT_False_Human                    1.000000   \n",
       "                     ML_D_10_RT_True_Human                     1.000000   \n",
       "                     Random_D_1_RT_True_Human                  0.025862   \n",
       "\n",
       "                                                                \\\n",
       "                                                           std   \n",
       "EnvName    Behaviour AgentId                                     \n",
       "FrozenLake Curated   ML_D_10_RT_False_Curated         0.000000   \n",
       "                     ML_D_10_RT_True_Curated          0.000000   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore  0.000000   \n",
       "                     ML_D_10_RT_False_HighScore       0.000000   \n",
       "                     ML_D_10_RT_True_HighScore        0.000000   \n",
       "                     Random_D_1_RT_True_HighScore     0.192952   \n",
       "           Human     HardCoded_D_1_RT_True_Human      0.000000   \n",
       "                     ML_D_10_RT_False_Human           0.000000   \n",
       "                     ML_D_10_RT_True_Human            0.000000   \n",
       "                     Random_D_1_RT_True_Human         0.159412   \n",
       "\n",
       "                                                     EpisodeTotalCuratedReward  \\\n",
       "                                                                          mean   \n",
       "EnvName    Behaviour AgentId                                                     \n",
       "FrozenLake Curated   ML_D_10_RT_False_Curated                         0.000000   \n",
       "                     ML_D_10_RT_True_Curated                          0.000000   \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore                  0.000000   \n",
       "                     ML_D_10_RT_False_HighScore                       0.000000   \n",
       "                     ML_D_10_RT_True_HighScore                        0.000000   \n",
       "                     Random_D_1_RT_True_HighScore                     0.124008   \n",
       "           Human     HardCoded_D_1_RT_True_Human                      0.000000   \n",
       "                     ML_D_10_RT_False_Human                           0.000000   \n",
       "                     ML_D_10_RT_True_Human                            0.000000   \n",
       "                     Random_D_1_RT_True_Human                         0.103448   \n",
       "\n",
       "                                                                \n",
       "                                                           std  \n",
       "EnvName    Behaviour AgentId                                    \n",
       "FrozenLake Curated   ML_D_10_RT_False_Curated         0.000000  \n",
       "                     ML_D_10_RT_True_Curated          0.000000  \n",
       "           HighScore HardCoded_D_1_RT_True_HighScore  0.000000  \n",
       "                     ML_D_10_RT_False_HighScore       0.000000  \n",
       "                     ML_D_10_RT_True_HighScore        0.000000  \n",
       "                     Random_D_1_RT_True_HighScore     0.329754  \n",
       "           Human     HardCoded_D_1_RT_True_Human      0.000000  \n",
       "                     ML_D_10_RT_False_Human           0.000000  \n",
       "                     ML_D_10_RT_True_Human            0.000000  \n",
       "                     Random_D_1_RT_True_Human         0.305865  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregateSettings = {}\n",
    "aggregateSettings[\"EpisodeId\"] = \"count\"\n",
    "aggregateSettings[\"Duration\"] = [\"mean\"]\n",
    "aggregateSettings[\"EpisodeTotalReward\"] = [\"mean\", \"std\"]\n",
    "aggregateSettings[\"EpisodeTotalCuratedReward\"] = [\"mean\", \"std\"]\n",
    "\n",
    "evalDf.groupby([\"EnvName\", \"Behaviour\", \"AgentId\"]).aggregate(aggregateSettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">EpisodeTotalReward</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th>AgentId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">FrozenLake</th>\n",
       "      <th>HardCoded_D_1_RT_True_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_False_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML_D_10_RT_True_HighScore</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random_D_1_RT_True_HighScore</th>\n",
       "      <td>0.03869</td>\n",
       "      <td>0.192952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           EpisodeTotalReward          \n",
       "                                                         mean       std\n",
       "EnvName    AgentId                                                     \n",
       "FrozenLake HardCoded_D_1_RT_True_HighScore            1.00000  0.000000\n",
       "           ML_D_10_RT_False_HighScore                 1.00000  0.000000\n",
       "           ML_D_10_RT_True_HighScore                  1.00000  0.000000\n",
       "           Random_D_1_RT_True_HighScore               0.03869  0.192952"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "evalDf[evalDf[\"Behaviour\"] == \"HighScore\"].groupby([\"EnvName\", \"AgentId\"])[[\"EpisodeTotalReward\"]].aggregate(aggregateSettings[\"EpisodeTotalReward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEnvAgentTypeTable(evalDf, envNames, agentTypes, metric):\n",
    "\n",
    "\tcolumns = [\"EnvName\"]\n",
    "\tcolumns += agentTypes\n",
    "\n",
    "\tdf = pd.DataFrame(columns=columns)\n",
    "\n",
    "\tfor envName in envNames:\n",
    "\t\trow = {}\n",
    "\t\trow[\"EnvName\"] = [envName]\n",
    "\n",
    "\t\tfor agentType in agentTypes:\n",
    "\t\t\tagentTypeDf = evalDf[evalDf[\"AgentType\"] == agentType]\n",
    "\t\t\tagentTypeDf = agentTypeDf[agentTypeDf[\"EnvName\"] == envName]\n",
    "\n",
    "\t\t\tavg = agentTypeDf[metric].mean()\n",
    "\t\t\terror = agentTypeDf[metric].std()\n",
    "\t\t\tcell = f\"{avg:.2f} ({error:.2f})\"\n",
    "\t\t\trow[agentType] = [cell]\n",
    "\n",
    "\t\tdf = pd.concat([df, pd.DataFrame(row)], ignore_index=True)\n",
    "\n",
    "\t\n",
    "\t# set the index to be the env name\n",
    "\tdf = df.set_index(\"EnvName\")\n",
    "\n",
    "\ttext = ConvertToLatex(df)\n",
    "\tpc.copy(text)\n",
    "\tprint(text.replace(\"\\n\", \" \"))\n",
    "\tdisplay(df)\n",
    "\tprint(\"Copied to clipboard\")\n",
    "\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evalDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m curatedDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCurated\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m humanDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHuman\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m highScoreDf \u001b[39m=\u001b[39m evalDf[evalDf[\u001b[39m\"\u001b[39m\u001b[39mBehaviour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHighScore\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evalDf' is not defined"
     ]
    }
   ],
   "source": [
    "curatedDf = evalDf[evalDf[\"Behaviour\"] == \"Curated\"]\n",
    "humanDf = evalDf[evalDf[\"Behaviour\"] == \"Human\"]\n",
    "highScoreDf = evalDf[evalDf[\"Behaviour\"] == \"HighScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t1.00 ±0.00 & 1.00 ±0.00 & 0.04 ±0.19 & nan ±nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>0.04 ±0.19</td>\n",
       "      <td>nan ±nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HardCoded          ML      Random     Human\n",
       "EnvName                                                 \n",
       "FrozenLake  1.00 ±0.00  1.00 ±0.00  0.04 ±0.19  nan ±nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(highScoreDf, EnvNames, AgentTypes, \"EpisodeTotalReward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t1.00 ±0.00 & 0.00 ±0.00 & 0.09 ±0.29 & nan ±nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>1.00 ±0.00</td>\n",
       "      <td>0.00 ±0.00</td>\n",
       "      <td>0.09 ±0.29</td>\n",
       "      <td>nan ±nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             HardCoded          ML      Random     Human\n",
       "EnvName                                                 \n",
       "FrozenLake  1.00 ±0.00  0.00 ±0.00  0.09 ±0.29  nan ±nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(curatedDf, EnvNames, AgentTypes, \"EpisodeTotalCuratedReward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t0.02 & 0.26 & 2.39 & nan \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.39</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           HardCoded    ML Random Human\n",
       "EnvName                                \n",
       "FrozenLake      0.02  0.26   2.39   nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(highScoreDf, EnvNames, AgentTypes, \"Duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690     9.979701\n",
       "691     7.378715\n",
       "692     6.106398\n",
       "693     2.483409\n",
       "694     1.422509\n",
       "          ...   \n",
       "4705    0.002112\n",
       "4706    0.002134\n",
       "4707    0.002237\n",
       "4708    0.002278\n",
       "4709    0.002250\n",
       "Name: TimePerStep, Length: 4008, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highScoreDf[\"TimePerStep\"] = highScoreDf[\"Duration\"] / highScoreDf[\"EpisodeSteps\"]\n",
    "\n",
    "highScoreDf[\"TimePerStep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{longtable}{|c|c|c|c|} \\caption{Insert Caption Here.} \\label{tab:InsertLabelHere} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endfirsthead  \\multicolumn{4}{c}% {{\\bfseries \\tablename\\ \\thetable{} -- continued from previous page}} \\\\ \\hline \t\\multicolumn{1}{|c|}{\\textbf{HardCoded}} & \t\\multicolumn{1}{c|}{\\textbf{ML}} & \t\\multicolumn{1}{c|}{\\textbf{Random}} & \t\\multicolumn{1}{c|}{\\textbf{Human}} \\\\ \\hline \\endhead  \\hline \\multicolumn{4}{|c|}{{Continued on next page}} \\\\ \\hline  \\endfoot \\hline \\endlastfoot  \t0.01 (0.00) & 0.06 (0.67) & 0.36 (0.12) & nan (nan) \\\\ \\hline \\end{longtable}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HardCoded</th>\n",
       "      <th>ML</th>\n",
       "      <th>Random</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FrozenLake</th>\n",
       "      <td>0.01 (0.00)</td>\n",
       "      <td>0.06 (0.67)</td>\n",
       "      <td>0.36 (0.12)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              HardCoded           ML       Random      Human\n",
       "EnvName                                                     \n",
       "FrozenLake  0.01 (0.00)  0.06 (0.67)  0.36 (0.12)  nan (nan)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to clipboard\n"
     ]
    }
   ],
   "source": [
    "CreateEnvAgentTypeTable(highScoreDf, EnvNames, AgentTypes, \"TimePerStep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "envName = \"CartPole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "envName = \"FrozenLake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>EpisodeId</th>\n",
       "      <th>Terminated</th>\n",
       "      <th>Truncated</th>\n",
       "      <th>EpisodeTotalReward</th>\n",
       "      <th>EpisodeTotalCuratedReward</th>\n",
       "      <th>EpisodeSteps</th>\n",
       "      <th>metricName</th>\n",
       "      <th>Agent</th>\n",
       "      <th>...</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_MaxDeploymentBatchSize</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_DataTable</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_ReplayExamples</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_LearningRate</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_DenseLayers</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_Activations</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_Dropout</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_L1</th>\n",
       "      <th>ModelConfigs_PlayStyle_Discriminator_L2</th>\n",
       "      <th>DataTables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1694554919748454300</td>\n",
       "      <td>1694554921488929900</td>\n",
       "      <td>2023-09-12_22-41-59-748</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>324.213065</td>\n",
       "      <td>500</td>\n",
       "      <td>HardCoded_D_1_T_1_RT_True_HighScore</td>\n",
       "      <td>HardCoded</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>PlayStyle_Trajectories</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[{'TableName': 'Forward_Trajectories', 'StepCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1694554922133490800</td>\n",
       "      <td>1694554924172616700</td>\n",
       "      <td>2023-09-12_22-42-02-133</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>341.217789</td>\n",
       "      <td>500</td>\n",
       "      <td>HardCoded_D_1_T_1_RT_True_HighScore</td>\n",
       "      <td>HardCoded</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>PlayStyle_Trajectories</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[{'TableName': 'Forward_Trajectories', 'StepCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1694554924890528600</td>\n",
       "      <td>1694554926591721100</td>\n",
       "      <td>2023-09-12_22-42-04-890</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>386.926498</td>\n",
       "      <td>500</td>\n",
       "      <td>HardCoded_D_1_T_1_RT_True_HighScore</td>\n",
       "      <td>HardCoded</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>PlayStyle_Trajectories</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[{'TableName': 'Forward_Trajectories', 'StepCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1694554927202768500</td>\n",
       "      <td>1694554928969381100</td>\n",
       "      <td>2023-09-12_22-42-07-202</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>358.574428</td>\n",
       "      <td>500</td>\n",
       "      <td>HardCoded_D_1_T_1_RT_True_HighScore</td>\n",
       "      <td>HardCoded</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>PlayStyle_Trajectories</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[{'TableName': 'Forward_Trajectories', 'StepCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1694554929588415700</td>\n",
       "      <td>1694554932137082700</td>\n",
       "      <td>2023-09-12_22-42-09-588</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>500.0</td>\n",
       "      <td>330.244899</td>\n",
       "      <td>500</td>\n",
       "      <td>HardCoded_D_1_T_1_RT_True_HighScore</td>\n",
       "      <td>HardCoded</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>PlayStyle_Trajectories</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[{'TableName': 'Forward_Trajectories', 'StepCo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartTime              EndTime                EpisodeId  \\\n",
       "0  1694554919748454300  1694554921488929900  2023-09-12_22-41-59-748   \n",
       "1  1694554922133490800  1694554924172616700  2023-09-12_22-42-02-133   \n",
       "2  1694554924890528600  1694554926591721100  2023-09-12_22-42-04-890   \n",
       "3  1694554927202768500  1694554928969381100  2023-09-12_22-42-07-202   \n",
       "4  1694554929588415700  1694554932137082700  2023-09-12_22-42-09-588   \n",
       "\n",
       "   Terminated  Truncated  EpisodeTotalReward  EpisodeTotalCuratedReward  \\\n",
       "0       False       True               500.0                 324.213065   \n",
       "1       False       True               500.0                 341.217789   \n",
       "2       False       True               500.0                 386.926498   \n",
       "3       False       True               500.0                 358.574428   \n",
       "4       False       True               500.0                 330.244899   \n",
       "\n",
       "   EpisodeSteps                           metricName      Agent  ...  \\\n",
       "0           500  HardCoded_D_1_T_1_RT_True_HighScore  HardCoded  ...   \n",
       "1           500  HardCoded_D_1_T_1_RT_True_HighScore  HardCoded  ...   \n",
       "2           500  HardCoded_D_1_T_1_RT_True_HighScore  HardCoded  ...   \n",
       "3           500  HardCoded_D_1_T_1_RT_True_HighScore  HardCoded  ...   \n",
       "4           500  HardCoded_D_1_T_1_RT_True_HighScore  HardCoded  ...   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_MaxDeploymentBatchSize  \\\n",
       "0                                               2048             \n",
       "1                                               2048             \n",
       "2                                               2048             \n",
       "3                                               2048             \n",
       "4                                               2048             \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_DataTable  \\\n",
       "0                          PlayStyle_Trajectories   \n",
       "1                          PlayStyle_Trajectories   \n",
       "2                          PlayStyle_Trajectories   \n",
       "3                          PlayStyle_Trajectories   \n",
       "4                          PlayStyle_Trajectories   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_ReplayExamples  \\\n",
       "0                                            curated     \n",
       "1                                            curated     \n",
       "2                                            curated     \n",
       "3                                            curated     \n",
       "4                                            curated     \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_LearningRate  \\\n",
       "0                                              0.001   \n",
       "1                                              0.001   \n",
       "2                                              0.001   \n",
       "3                                              0.001   \n",
       "4                                              0.001   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_DenseLayers  \\\n",
       "0                                             [256]   \n",
       "1                                             [256]   \n",
       "2                                             [256]   \n",
       "3                                             [256]   \n",
       "4                                             [256]   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_Activations  \\\n",
       "0                                              relu   \n",
       "1                                              relu   \n",
       "2                                              relu   \n",
       "3                                              relu   \n",
       "4                                              relu   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_Dropout  \\\n",
       "0                                           0.5   \n",
       "1                                           0.5   \n",
       "2                                           0.5   \n",
       "3                                           0.5   \n",
       "4                                           0.5   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_L1  \\\n",
       "0                                      0.5   \n",
       "1                                      0.5   \n",
       "2                                      0.5   \n",
       "3                                      0.5   \n",
       "4                                      0.5   \n",
       "\n",
       "   ModelConfigs_PlayStyle_Discriminator_L2  \\\n",
       "0                                      0.5   \n",
       "1                                      0.5   \n",
       "2                                      0.5   \n",
       "3                                      0.5   \n",
       "4                                      0.5   \n",
       "\n",
       "                                          DataTables  \n",
       "0  [{'TableName': 'Forward_Trajectories', 'StepCo...  \n",
       "1  [{'TableName': 'Forward_Trajectories', 'StepCo...  \n",
       "2  [{'TableName': 'Forward_Trajectories', 'StepCo...  \n",
       "3  [{'TableName': 'Forward_Trajectories', 'StepCo...  \n",
       "4  [{'TableName': 'Forward_Trajectories', 'StepCo...  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runGroup = \"dev\"\n",
    "agent = \"HardCoded\"\n",
    "stats = pd.read_csv(f\"Data//{envName}//{runGroup}//replays//{agent}//stats.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"Duration\"] = (stats[\"EndTime\"] - stats[\"StartTime\"])  / 1e9\n",
    "stats[\"DurationPerStep\"] = stats[\"Duration\"] / stats[\"EpisodeSteps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for HardCoded in CartPole\n",
      "EpisodeTotalReward            : 500.000 (0.000)\n",
      "EpisodeTotalCuratedReward     : 347.596 (57.922)\n",
      "DurationPerStep               : 0.004 (0.001)\n"
     ]
    }
   ],
   "source": [
    "keys = [\"EpisodeTotalReward\", \"EpisodeTotalCuratedReward\", \"DurationPerStep\"]\n",
    "\n",
    "print(f\"Stats for {agent} in {envName}\")\n",
    "\n",
    "for key in keys:\n",
    "\n",
    "\tvalues = stats[key].values\n",
    "\n",
    "\tprint(f\"{key:<30}: {values.mean():.3f} ({values.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 105 fields in line 202, saw 109\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\ResultsFormating.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stats \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mData//FrozenLake//dev//replays//HardCoded//stats.tsv\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m logName \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mloggerName\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louie/Documents/Git/DECAF/ResultsFormating.ipynb#Y150sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmetricName\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m stats\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 105 fields in line 202, saw 109\n"
     ]
    }
   ],
   "source": [
    "stats = pd.read_csv(f\"Data//FrozenLake//dev//replays//HardCoded//stats.tsv\", sep=\"\\t\")\n",
    "\n",
    "logName = \"loggerName\"\n",
    "if \"metricName\" in stats.columns:\n",
    "\tlogName = \"metricName\"\n",
    "stats[logName].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviourMatch = stats[logName].str.lower().str.contains(\"curated\")\n",
    "\n",
    "temp = stats[behaviourMatch]\n",
    "temp = temp[temp[\"Agent\"] == \"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats for Human in FrozenLake runGroup: dev\n",
      "EpisodeTotalReward            : 0.96 (0.19)\n",
      "EpisodeTotalCuratedReward     : 1.00 (0.00)\n",
      "DurationPerStep               : 0.41 (0.24)\n",
      "\n",
      "Stats for ML in CartPole runGroup: dev\n",
      "EpisodeTotalReward            : 496.60 (10.20)\n",
      "EpisodeTotalCuratedReward     : nan (nan)\n",
      "DurationPerStep               : 0.25 (0.06)\n",
      "\n",
      "Stats for Random in CartPole runGroup: dev\n",
      "EpisodeTotalReward            : 22.08 (11.60)\n",
      "EpisodeTotalCuratedReward     : nan (nan)\n",
      "DurationPerStep               : 0.07 (0.02)\n",
      "\n",
      "Stats for Human in CartPole runGroup: dev\n",
      "EpisodeTotalReward            : 189.00 (139.51)\n",
      "EpisodeTotalCuratedReward     : 0.78 (0.07)\n",
      "DurationPerStep               : 0.43 (0.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\AppData\\Local\\Temp\\ipykernel_31440\\2171892634.py:36: RuntimeWarning: Mean of empty slice.\n",
      "  print(f\"{key:<30}: {values.mean():.2f} ({values.std():.2f})\")\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\louie\\AppData\\Local\\Temp\\ipykernel_31440\\2171892634.py:36: RuntimeWarning: Mean of empty slice.\n",
      "  print(f\"{key:<30}: {values.mean():.2f} ({values.std():.2f})\")\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "c:\\Users\\louie\\Documents\\Git\\DECAF\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "keys = [\"EpisodeTotalReward\", \"EpisodeTotalCuratedReward\", \"DurationPerStep\"]\n",
    "# keys = [\"EpisodeTotalCuratedReward\"]\n",
    "\n",
    "runGroup = \"dev\"\n",
    "for envName in EnvNames:\n",
    "\tfor agent in AgentTypes:\n",
    "\t\ttry:\n",
    "\t\t\tstats = pd.read_csv(f\"Data//{envName}//{runGroup}//replays//{agent}//stats.tsv\", sep=\"\\t\")\n",
    "\t\t\n",
    "\n",
    "\t\t\tstats[\"Duration\"] = (stats[\"EndTime\"] - stats[\"StartTime\"])  / 1e9\n",
    "\t\t\tstats[\"DurationPerStep\"] = stats[\"Duration\"] / stats[\"EpisodeSteps\"]\n",
    "\n",
    "\t\t\tif envName == \"CartPole\":\n",
    "\t\t\t\tstats[\"EpisodeTotalCuratedReward\"] = stats[\"EpisodeTotalCuratedReward\"] / stats[\"EpisodeSteps\"]\n",
    "\n",
    "\t\t\tprint()\n",
    "\t\t\tprint(f\"Stats for {agent} in {envName} runGroup: {runGroup}\")\n",
    "\n",
    "\t\t\tfor key in keys:\n",
    "\n",
    "\t\t\t\tvalues = stats[key].values\n",
    "\n",
    "\n",
    "\t\t\t\tif key == \"EpisodeTotalCuratedReward\":\n",
    "\t\t\t\t\tlogName = \"loggerName\"\n",
    "\t\t\t\t\tif \"metricName\" in stats.columns:\n",
    "\t\t\t\t\t\tlogName = \"metricName\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tmetricNames = stats[logName].str.lower()\n",
    "\t\t\t\t\tbehaviourMatch = metricNames.str.contains(\"curated\")\n",
    "\n",
    "\t\t\t\t\tvalues = stats[behaviourMatch][key].values\n",
    "\n",
    "\n",
    "\t\t\t\tprint(f\"{key:<30}: {values.mean():.2f} ({values.std():.2f})\")\n",
    "\n",
    "\t\t\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t# print(f\"Failed to load stats for {agent} in {envName} {e}\")\n",
    "\t\t\tpass\n",
    "\t\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
